{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Tuto 1 - First steps, Creating a RDD\n",
    "\n",
    "The first notebook was just a teaser to get you interested in PySpark.\n",
    "In this second notebook we will look in a little more details at the spark structure and how to create Resilient Distributed Datasets (RDD).\n",
    "\n",
    "__Important Note__ :\n",
    "I didn't want to mention it in the first notebook but if you are familiar with python when running `sc.parallelize()` you probably wondered where does the `sc` come from.\n",
    "Actually we are running the notebook in a Spark shell in which a `SparkContext` object has already been created and named `sc`, look :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.1.78.28:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SparkContext` object is the first thing to create using a `SparkConf` object as well but you won't have to worry about it here as it is automatically created in our Spark shell.\n",
    "Keep this in mind if you decide to venture outside of the shell.\n",
    "\n",
    "## Creating RDD\n",
    "\n",
    "RDDs are the central part of PySpark.\n",
    "Once created we can apply different operations to each rdd which are separated into two groups : _transformations_ and _actions_ (we'll se later the differences).  \n",
    "But first we need to create RDDs...\n",
    "\n",
    "There are two ways of creating rdds, using `parallelize` or creating it from external files.\n",
    "\n",
    "### Using `sc.parallelize()`\n",
    "\n",
    "Exactly like we did in the previous notebook :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_samples = 1000000\n",
    "rdd = sc.parallelize(np.arange(0,n_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:480"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when we call the function `sc.parallelize` spark automatically chose the number of partitions, aka in how many pieces the dataset is distributed. According to the [programming guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html) : __\"Typically you want 2-4 partitions for each CPU in your cluster\"__  \n",
    "We can check in how many partitions or dataset has been divided :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is anyway possible to force the number of partitions when creating the RDD :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(np.arange(0,n_samples), 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using External Files\n",
    "\n",
    "To create RDD it is also possible to use external files and any storage source supported by Hadoop.\n",
    "The main function to create a rdd from a file is named `sc.textFile`.\n",
    "\n",
    "Hereafter we'll simply download a dataset and create a rdd from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('adult.data', <http.client.HTTPMessage at 0x106579320>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# downloading the dataset (can take some time)\n",
    "urllib.request.urlretrieve('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data', 'adult.data')\n",
    "urllib.request.urlretrieve('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names', 'adult.names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# creating the rdd from the adult dataset\n",
    "data_file = 'adult.data'\n",
    "adult_rdd = sc.textFile(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look the first 5 entries of the dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K',\n",
       " '50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K',\n",
       " '38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, <=50K',\n",
       " '53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, Male, 0, 0, 40, United-States, <=50K',\n",
       " '28, Private, 338409, Bachelors, 13, Married-civ-spouse, Prof-specialty, Wife, Black, Female, 0, 0, 40, Cuba, <=50K']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage we have only loaded the data in a rdd and we can see that it is loaded as a list of long strings, one per line in the original file.\n",
    "I can also already tell you that there are missing data in the dataset.  \n",
    "All of this will be the opportunity in the next notebook to look at the different _operations_ we can perform on a rdd and the difference between _transformations_ and _actions_.\n",
    "\n",
    "### Notes on Partitions and Repartitioning\n",
    "\n",
    "When using `sc.textFile` pyspark choose by itself how many partitions should be created.\n",
    "If we want a different number of partitions we can proceed in several different ways :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Force the minimum number of partitions when calling sc.textFile\n",
    "adult_rdd = sc.textFile(data_file, minPartitions=4)\n",
    "adult_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Repartition the rdd (include a shuffling before repartition)\n",
    "adult_rdd = sc.textFile(data_file).repartition(6)\n",
    "adult_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "__Note__ : If you want to reduce the number of partition of partitions it is actually recommanded to use a different function named `coalesce` (doesn't work to increase the number of partitions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult_rdd = sc.textFile(data_file).coalesce(1)\n",
    "adult_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
